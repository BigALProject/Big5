{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.0.0\n",
      "Eager mode:  True\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# Revised for our dataset from:\n",
    "# https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_mlp_cora\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import neural_structured_learning as nsl\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Resets notebook state\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "### Experiment dataset\n",
    "TRAIN_DATA_PATH = '../../../data/cora/cora/train_merged_examples.tfr'\n",
    "TEST_DATA_PATH = '../../../data/cora/cora/test_examples.tfr'\n",
    "\n",
    "### Constants used to identify neighbor features in the input.\n",
    "NBR_FEATURE_PREFIX = 'NL_nbr_'\n",
    "NBR_WEIGHT_SUFFIX = '_weight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class HParams(object):\n",
    "  \"\"\"Hyperparameters used for training.\"\"\"\n",
    "  def __init__(self):\n",
    "    ### dataset parameters\n",
    "    self.num_classes = 7\n",
    "    self.max_seq_length = 1433\n",
    "    ### neural graph learning parameters\n",
    "    self.distance_type = nsl.configs.DistanceType.L2\n",
    "    self.graph_regularization_multiplier = 0.1\n",
    "    self.num_neighbors = 1\n",
    "    ### model architecture\n",
    "    self.num_fc_units = [50, 50]\n",
    "    ### training parameters\n",
    "    self.train_epochs = 100\n",
    "    self.batch_size = 128\n",
    "    self.dropout_rate = 0.5\n",
    "    ### eval parameters\n",
    "    self.eval_steps = None  # All instances in the test set are evaluated.\n",
    "\n",
    "HPARAMS = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def parse_example(example_proto):\n",
    "  \"\"\"Extracts relevant fields from the `example_proto`.\n",
    "\n",
    "  Args:\n",
    "    example_proto: An instance of `tf.train.Example`.\n",
    "\n",
    "  Returns:\n",
    "    A pair whose first value is a dictionary containing relevant features\n",
    "    and whose second value contains the ground truth labels.\n",
    "  \"\"\"\n",
    "  # The 'words' feature is a multi-hot, bag-of-words representation of the\n",
    "  # original raw text. A default value is required for examples that don't\n",
    "  # have the feature.\n",
    "  feature_spec = {\n",
    "      'words':\n",
    "          tf.io.FixedLenFeature([HPARAMS.max_seq_length],\n",
    "                                tf.int64,\n",
    "                                default_value=tf.constant(\n",
    "                                    0,\n",
    "                                    dtype=tf.int64,\n",
    "                                    shape=[HPARAMS.max_seq_length])),\n",
    "      'label':\n",
    "          tf.io.FixedLenFeature((), tf.int64, default_value=-1),\n",
    "  }\n",
    "  # We also extract corresponding neighbor features in a similar manner to\n",
    "  # the features above.\n",
    "  for i in range(HPARAMS.num_neighbors):\n",
    "    nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
    "    nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, i, NBR_WEIGHT_SUFFIX)\n",
    "    feature_spec[nbr_feature_key] = tf.io.FixedLenFeature(\n",
    "        [HPARAMS.max_seq_length],\n",
    "        tf.int64,\n",
    "        default_value=tf.constant(\n",
    "            0, dtype=tf.int64, shape=[HPARAMS.max_seq_length]))\n",
    "\n",
    "    # We assign a default value of 0.0 for the neighbor weight so that\n",
    "    # graph regularization is done on samples based on their exact number\n",
    "    # of neighbors. In other words, non-existent neighbors are discounted.\n",
    "    feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n",
    "        [1], tf.float32, default_value=tf.constant([0.0]))\n",
    "\n",
    "  features = tf.io.parse_single_example(example_proto, feature_spec)\n",
    "\n",
    "  labels = features.pop('label')\n",
    "  return features, labels\n",
    "\n",
    "\n",
    "def make_dataset(file_path, training=False):\n",
    "  \"\"\"Creates a `tf.data.TFRecordDataset`.\n",
    "\n",
    "  Args:\n",
    "    file_path: Name of the file in the `.tfrecord` format containing\n",
    "      `tf.train.Example` objects.\n",
    "    training: Boolean indicating if we are in training mode.\n",
    "\n",
    "  Returns:\n",
    "    An instance of `tf.data.TFRecordDataset` containing the `tf.train.Example`\n",
    "    objects.\n",
    "  \"\"\"\n",
    "  dataset = tf.data.TFRecordDataset([file_path])\n",
    "  if training:\n",
    "    dataset = dataset.shuffle(10000)\n",
    "  dataset = dataset.map(parse_example)\n",
    "  dataset = dataset.batch(HPARAMS.batch_size)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "train_dataset = make_dataset(TRAIN_DATA_PATH, training=True)\n",
    "test_dataset = make_dataset(TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ({NL_nbr_0_weight: (None, 1), NL_nbr_0_words: (None, 1433), words: (None, 1433)}, (None,)), types: ({NL_nbr_0_weight: tf.float32, NL_nbr_0_words: tf.int64, words: tf.int64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature list: ['NL_nbr_0_weight', 'NL_nbr_0_words', 'words']\n",
      "Batch of inputs: tf.Tensor(\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]], shape=(128, 1433), dtype=int64)\n",
      "Batch of neighbor inputs: tf.Tensor(\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]], shape=(128, 1433), dtype=int64)\n",
      "Batch of neighbor weights: tf.Tensor(\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "Batch of labels: tf.Tensor(\n",
      "[1 2 2 0 0 2 3 6 2 3 3 4 0 0 0 3 4 2 0 6 2 6 3 0 4 6 2 1 4 1 2 4 2 3 0 3 6\n",
      " 0 2 0 2 2 2 1 2 3 2 1 6 2 4 3 2 4 0 6 1 6 3 2 2 2 6 6 2 0 1 1 0 2 2 2 3 1\n",
      " 0 2 0 5 2 0 6 1 3 4 0 2 1 6 1 4 2 1 6 2 2 0 4 0 0 6 1 6 2 2 5 3 5 2 2 3 1\n",
      " 6 2 0 3 1 5 1 2 2 6 0 3 2 1 0 3 3], shape=(128,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_dataset.take(1):\n",
    "  print('Feature list:', list(feature_batch.keys()))\n",
    "  print('Batch of inputs:', feature_batch['words'])\n",
    "  nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, 0, 'words')\n",
    "  nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, 0, NBR_WEIGHT_SUFFIX)\n",
    "  print('Batch of neighbor inputs:', feature_batch[nbr_feature_key])\n",
    "  print('Batch of neighbor weights:',\n",
    "        tf.reshape(feature_batch[nbr_weight_key], [-1]))\n",
    "  print('Batch of labels:', label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature list: ['NL_nbr_0_weight', 'NL_nbr_0_words', 'words']\n",
      "Batch of inputs: tf.Tensor(\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]], shape=(128, 1433), dtype=int64)\n",
      "Batch of neighbor inputs: tf.Tensor(\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]], shape=(128, 1433), dtype=int64)\n",
      "Batch of neighbor weights: tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "Batch of labels: tf.Tensor(\n",
      "[5 2 2 2 1 2 6 3 2 3 6 1 3 6 4 4 2 3 3 0 2 0 5 2 1 0 6 3 6 4 2 2 3 0 4 2 2\n",
      " 2 2 3 2 2 2 0 2 2 2 2 4 2 3 4 0 2 6 2 1 4 2 0 0 1 4 2 6 0 5 2 2 3 2 5 2 5\n",
      " 2 3 2 2 2 2 2 6 6 3 2 4 2 6 3 2 2 6 2 4 2 2 1 3 4 6 0 0 2 4 2 1 3 6 6 2 6\n",
      " 6 6 1 4 6 4 3 6 6 0 0 2 6 2 4 0 0], shape=(128,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in test_dataset.take(1):\n",
    "  print('Feature list:', list(feature_batch.keys()))\n",
    "  print('Batch of inputs:', feature_batch['words'])\n",
    "  nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, 0, 'words')\n",
    "  nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, 0, NBR_WEIGHT_SUFFIX)\n",
    "  print('Batch of neighbor inputs:', feature_batch[nbr_feature_key])\n",
    "  print('Batch of neighbor weights:',\n",
    "        tf.reshape(feature_batch[nbr_weight_key], [-1]))\n",
    "  print('Batch of labels:', label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# In order to demonstrate the use of graph regularization, we build a base model for this problem first. We will use a simple feed-forward neural network with 2 hidden layers and dropout in between. We illustrate the creation of the base model using all model types supported by the tf.Keras framework -- sequential, functional, and subclass.\n",
    "\n",
    "# Sequential base model\n",
    "def make_mlp_sequential_model(hparams):\n",
    "  \"\"\"Creates a sequential multi-layer perceptron model.\"\"\"\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(\n",
    "      tf.keras.layers.InputLayer(\n",
    "          input_shape=(hparams.max_seq_length,), name='words'))\n",
    "  # Input is already one-hot encoded in the integer format. We cast it to\n",
    "  # floating point format here.\n",
    "  model.add(\n",
    "      tf.keras.layers.Lambda(lambda x: tf.keras.backend.cast(x, tf.float32)))\n",
    "  for num_units in hparams.num_fc_units:\n",
    "    model.add(tf.keras.layers.Dense(num_units, activation='relu'))\n",
    "    # For sequential models, by default, Keras ensures that the 'dropout' layer\n",
    "    # is invoked only during training.\n",
    "    model.add(tf.keras.layers.Dropout(hparams.dropout_rate))\n",
    "  model.add(tf.keras.layers.Dense(hparams.num_classes, activation='softmax'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def make_mlp_functional_model(hparams):\n",
    "  \"\"\"Creates a functional API-based multi-layer perceptron model.\"\"\"\n",
    "  inputs = tf.keras.Input(\n",
    "      shape=(hparams.max_seq_length,), dtype='int64', name='words')\n",
    "\n",
    "  # Input is already one-hot encoded in the integer format. We cast it to\n",
    "  # floating point format here.\n",
    "  cur_layer = tf.keras.layers.Lambda(\n",
    "      lambda x: tf.keras.backend.cast(x, tf.float32))(\n",
    "          inputs)\n",
    "\n",
    "  for num_units in hparams.num_fc_units:\n",
    "    cur_layer = tf.keras.layers.Dense(num_units, activation='relu')(cur_layer)\n",
    "    # For functional models, by default, Keras ensures that the 'dropout' layer\n",
    "    # is invoked only during training.\n",
    "    cur_layer = tf.keras.layers.Dropout(hparams.dropout_rate)(cur_layer)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(\n",
    "      hparams.num_classes, activation='softmax')(\n",
    "          cur_layer)\n",
    "\n",
    "  model = tf.keras.Model(inputs, outputs=outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def make_mlp_subclass_model(hparams):\n",
    "  \"\"\"Creates a multi-layer perceptron subclass model in Keras.\"\"\"\n",
    "\n",
    "  class MLP(tf.keras.Model):\n",
    "    \"\"\"Subclass model defining a multi-layer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "      super(MLP, self).__init__()\n",
    "      # Input is already one-hot encoded in the integer format. We create a\n",
    "      # layer to cast it to floating point format here.\n",
    "      self.cast_to_float_layer = tf.keras.layers.Lambda(\n",
    "          lambda x: tf.keras.backend.cast(x, tf.float32))\n",
    "      self.dense_layers = [\n",
    "          tf.keras.layers.Dense(num_units, activation='relu')\n",
    "          for num_units in hparams.num_fc_units\n",
    "      ]\n",
    "      self.dropout_layer = tf.keras.layers.Dropout(hparams.dropout_rate)\n",
    "      self.output_layer = tf.keras.layers.Dense(\n",
    "          hparams.num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "      cur_layer = self.cast_to_float_layer(inputs['words'])\n",
    "      for dense_layer in self.dense_layers:\n",
    "        cur_layer = dense_layer(cur_layer)\n",
    "        cur_layer = self.dropout_layer(cur_layer, training=training)\n",
    "\n",
    "      outputs = self.output_layer(cur_layer)\n",
    "\n",
    "      return outputs\n",
    "\n",
    "  return MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "words (InputLayer)           [(None, 1433)]            0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 1433)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                71700     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 74,607\n",
      "Trainable params: 74,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a base MLP model using the functional API.\n",
    "# Alternatively, you can also create a sequential or subclass base model using\n",
    "# the make_mlp_sequential_model() or make_mlp_subclass_model() functions\n",
    "# respectively, defined above. Note that if a subclass model is used, its\n",
    "# summary cannot be generated until it is built.\n",
    "base_model_tag, base_model = 'FUNCTIONAL', make_mlp_functional_model(HPARAMS)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 1s 57ms/step - loss: 1.9214 - accuracy: 0.2074\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 1.8307 - accuracy: 0.3063\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 1.7388 - accuracy: 0.3397\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 1.6182 - accuracy: 0.3870\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 1.4534 - accuracy: 0.4719\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 1.3046 - accuracy: 0.5244\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 1.1951 - accuracy: 0.5814\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 1.0647 - accuracy: 0.6464\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.9450 - accuracy: 0.6919\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.8485 - accuracy: 0.7336\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.7605 - accuracy: 0.7592\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.7023 - accuracy: 0.7703\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6362 - accuracy: 0.7916\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6004 - accuracy: 0.8135\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5280 - accuracy: 0.8427\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5101 - accuracy: 0.8464\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4471 - accuracy: 0.8654\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4252 - accuracy: 0.8608\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.4157 - accuracy: 0.8710\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3709 - accuracy: 0.8923\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3501 - accuracy: 0.8928\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3251 - accuracy: 0.9030\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3078 - accuracy: 0.9109\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3057 - accuracy: 0.8993\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.2974 - accuracy: 0.9104\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2555 - accuracy: 0.9193\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.2536 - accuracy: 0.9262\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.2274 - accuracy: 0.9304\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2402 - accuracy: 0.9281\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.2086 - accuracy: 0.9439\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.2075 - accuracy: 0.9401\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1838 - accuracy: 0.9471\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.2009 - accuracy: 0.9332\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1898 - accuracy: 0.9383\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1824 - accuracy: 0.9531\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1674 - accuracy: 0.9503\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1572 - accuracy: 0.9541\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1740 - accuracy: 0.9494\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1445 - accuracy: 0.9578\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1571 - accuracy: 0.9568\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1487 - accuracy: 0.9573\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1430 - accuracy: 0.9582\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1250 - accuracy: 0.9633\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1275 - accuracy: 0.9615\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1306 - accuracy: 0.9647\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1115 - accuracy: 0.9684\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1169 - accuracy: 0.9717\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1135 - accuracy: 0.9717\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.1130 - accuracy: 0.9661\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1155 - accuracy: 0.9647\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1013 - accuracy: 0.9740\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.1035 - accuracy: 0.9712\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1026 - accuracy: 0.9708\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0986 - accuracy: 0.9726\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0994 - accuracy: 0.9731\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0917 - accuracy: 0.9773\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0839 - accuracy: 0.9777\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0743 - accuracy: 0.9819\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0988 - accuracy: 0.9717\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0828 - accuracy: 0.9759\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0843 - accuracy: 0.9782\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0779 - accuracy: 0.9791\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0785 - accuracy: 0.9796\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0798 - accuracy: 0.9768\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0739 - accuracy: 0.9777\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0753 - accuracy: 0.9828\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0711 - accuracy: 0.9777\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0766 - accuracy: 0.9800\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0645 - accuracy: 0.9805\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0692 - accuracy: 0.9791\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0589 - accuracy: 0.9833\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0646 - accuracy: 0.9810\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0775 - accuracy: 0.9791\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0664 - accuracy: 0.9773\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0630 - accuracy: 0.9824\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0641 - accuracy: 0.9796\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0660 - accuracy: 0.9814\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0550 - accuracy: 0.9852\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0618 - accuracy: 0.9824\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0658 - accuracy: 0.9805\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0490 - accuracy: 0.9879\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0636 - accuracy: 0.9824\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0469 - accuracy: 0.9889\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0648 - accuracy: 0.9796\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0539 - accuracy: 0.9838\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0608 - accuracy: 0.9810\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0599 - accuracy: 0.9819\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0528 - accuracy: 0.9847\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0537 - accuracy: 0.9861\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0451 - accuracy: 0.9898\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0498 - accuracy: 0.9842\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0555 - accuracy: 0.9842\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0464 - accuracy: 0.9875\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0574 - accuracy: 0.9810\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0540 - accuracy: 0.9828\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0452 - accuracy: 0.9870\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0484 - accuracy: 0.9847\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0501 - accuracy: 0.9865\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0515 - accuracy: 0.9838\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0479 - accuracy: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x205b42ed198>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and train the base MLP model\n",
    "base_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "base_model.fit(train_dataset, epochs=HPARAMS.train_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 23ms/step - loss: 1.2874 - accuracy: 0.7939\n",
      "\n",
      "\n",
      "Eval accuracy for  Base MLP model :  0.79385173\n",
      "Eval loss for  Base MLP model :  1.2873695850372315\n"
     ]
    }
   ],
   "source": [
    "# Helper function to print evaluation metrics.\n",
    "def print_metrics(model_desc, eval_metrics):\n",
    "  \"\"\"Prints evaluation metrics.\n",
    "\n",
    "  Args:\n",
    "    model_desc: A description of the model.\n",
    "    eval_metrics: A dictionary mapping metric names to corresponding values. It\n",
    "      must contain the loss and accuracy metrics.\n",
    "  \"\"\"\n",
    "  print('\\n')\n",
    "  print('Eval accuracy for ', model_desc, ': ', eval_metrics['accuracy'])\n",
    "  print('Eval loss for ', model_desc, ': ', eval_metrics['loss'])\n",
    "  if 'graph_loss' in eval_metrics:\n",
    "    print('Eval graph loss for ', model_desc, ': ', eval_metrics['graph_loss'])\n",
    "\n",
    "eval_results = dict(\n",
    "    zip(base_model.metrics_names,\n",
    "        base_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)))\n",
    "print_metrics('Base MLP model', eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashley\\pycharmprojects\\big5\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "c:\\users\\ashley\\pycharmprojects\\big5\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 79ms/step - loss: 1.9065 - accuracy: 0.2028 - graph_loss: 0.0083\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 1.8221 - accuracy: 0.3030 - graph_loss: 0.0143\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 1.7291 - accuracy: 0.3536 - graph_loss: 0.0289\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 1.6103 - accuracy: 0.4000 - graph_loss: 0.0442\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 1.4840 - accuracy: 0.4733 - graph_loss: 0.0728\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 1.3683 - accuracy: 0.5313 - graph_loss: 0.1066\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 1.2370 - accuracy: 0.5968 - graph_loss: 0.1420\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 1.0889 - accuracy: 0.6469 - graph_loss: 0.1820\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.9922 - accuracy: 0.6905 - graph_loss: 0.2180\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.8694 - accuracy: 0.7401 - graph_loss: 0.2433\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.8005 - accuracy: 0.7466 - graph_loss: 0.2611\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.7279 - accuracy: 0.7838 - graph_loss: 0.2774\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6950 - accuracy: 0.7884 - graph_loss: 0.2887\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6125 - accuracy: 0.8255 - graph_loss: 0.2911\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5726 - accuracy: 0.8376 - graph_loss: 0.3113\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5611 - accuracy: 0.8399 - graph_loss: 0.3135\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4838 - accuracy: 0.8571 - graph_loss: 0.3027\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.4627 - accuracy: 0.8761 - graph_loss: 0.3235\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4339 - accuracy: 0.8729 - graph_loss: 0.3263\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.4314 - accuracy: 0.8821 - graph_loss: 0.3359\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3738 - accuracy: 0.9044 - graph_loss: 0.3121\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3385 - accuracy: 0.9123 - graph_loss: 0.3308\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3365 - accuracy: 0.9151 - graph_loss: 0.3251\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3247 - accuracy: 0.9155 - graph_loss: 0.3396\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2993 - accuracy: 0.9220 - graph_loss: 0.3351\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2855 - accuracy: 0.9248 - graph_loss: 0.3381\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2781 - accuracy: 0.9262 - graph_loss: 0.3430\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2739 - accuracy: 0.9332 - graph_loss: 0.3483\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2496 - accuracy: 0.9411 - graph_loss: 0.3359\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.2462 - accuracy: 0.9411 - graph_loss: 0.3384\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.2340 - accuracy: 0.9434 - graph_loss: 0.3391\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2352 - accuracy: 0.9485 - graph_loss: 0.3435\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2025 - accuracy: 0.9582 - graph_loss: 0.3323\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.2111 - accuracy: 0.9531 - graph_loss: 0.3422\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1920 - accuracy: 0.9615 - graph_loss: 0.3265\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1900 - accuracy: 0.9592 - graph_loss: 0.3507\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1763 - accuracy: 0.9619 - graph_loss: 0.3275\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1945 - accuracy: 0.9578 - graph_loss: 0.3489\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1818 - accuracy: 0.9596 - graph_loss: 0.3468\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1792 - accuracy: 0.9592 - graph_loss: 0.3365\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1649 - accuracy: 0.9657 - graph_loss: 0.3408\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1621 - accuracy: 0.9657 - graph_loss: 0.3330\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1638 - accuracy: 0.9643 - graph_loss: 0.3482\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1598 - accuracy: 0.9680 - graph_loss: 0.3395\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1455 - accuracy: 0.9745 - graph_loss: 0.3448\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1510 - accuracy: 0.9698 - graph_loss: 0.3492\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1436 - accuracy: 0.9698 - graph_loss: 0.3415\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1403 - accuracy: 0.9735 - graph_loss: 0.3453\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1493 - accuracy: 0.9722 - graph_loss: 0.3454\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1327 - accuracy: 0.9782 - graph_loss: 0.3470\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1311 - accuracy: 0.9722 - graph_loss: 0.3360\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1206 - accuracy: 0.9763 - graph_loss: 0.3428\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1250 - accuracy: 0.9735 - graph_loss: 0.3410\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1338 - accuracy: 0.9735 - graph_loss: 0.3381\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1194 - accuracy: 0.9749 - graph_loss: 0.3486\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1279 - accuracy: 0.9712 - graph_loss: 0.3451\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1265 - accuracy: 0.9754 - graph_loss: 0.3495\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1131 - accuracy: 0.9805 - graph_loss: 0.3501\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1148 - accuracy: 0.9819 - graph_loss: 0.3531\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1165 - accuracy: 0.9814 - graph_loss: 0.3443\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1121 - accuracy: 0.9763 - graph_loss: 0.3491\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1238 - accuracy: 0.9768 - graph_loss: 0.3400\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1154 - accuracy: 0.9787 - graph_loss: 0.3320\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1109 - accuracy: 0.9791 - graph_loss: 0.3437\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1045 - accuracy: 0.9861 - graph_loss: 0.3416\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1045 - accuracy: 0.9791 - graph_loss: 0.3387\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1075 - accuracy: 0.9800 - graph_loss: 0.3454\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1045 - accuracy: 0.9787 - graph_loss: 0.3437\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1060 - accuracy: 0.9819 - graph_loss: 0.3392\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0973 - accuracy: 0.9800 - graph_loss: 0.3365\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1082 - accuracy: 0.9810 - graph_loss: 0.3416\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1068 - accuracy: 0.9787 - graph_loss: 0.3442\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0932 - accuracy: 0.9875 - graph_loss: 0.3377\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0981 - accuracy: 0.9856 - graph_loss: 0.3452\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0967 - accuracy: 0.9833 - graph_loss: 0.3476\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0975 - accuracy: 0.9828 - graph_loss: 0.3445\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0898 - accuracy: 0.9842 - graph_loss: 0.3375\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0914 - accuracy: 0.9856 - graph_loss: 0.3441\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0882 - accuracy: 0.9884 - graph_loss: 0.3447\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0960 - accuracy: 0.9819 - graph_loss: 0.3392\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0869 - accuracy: 0.9861 - graph_loss: 0.3363\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1002 - accuracy: 0.9824 - graph_loss: 0.3451\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0915 - accuracy: 0.9833 - graph_loss: 0.3358\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0873 - accuracy: 0.9879 - graph_loss: 0.3388\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0883 - accuracy: 0.9879 - graph_loss: 0.3402\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0836 - accuracy: 0.9865 - graph_loss: 0.3404\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0804 - accuracy: 0.9875 - graph_loss: 0.3374\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0858 - accuracy: 0.9870 - graph_loss: 0.3385\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0836 - accuracy: 0.9870 - graph_loss: 0.3371\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0783 - accuracy: 0.9884 - graph_loss: 0.3339\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0770 - accuracy: 0.9893 - graph_loss: 0.3429\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0814 - accuracy: 0.9916 - graph_loss: 0.3401\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0817 - accuracy: 0.9856 - graph_loss: 0.3425\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0821 - accuracy: 0.9856 - graph_loss: 0.3398\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0790 - accuracy: 0.9875 - graph_loss: 0.3445\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0795 - accuracy: 0.9889 - graph_loss: 0.3388\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0861 - accuracy: 0.9842 - graph_loss: 0.3412\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0768 - accuracy: 0.9889 - graph_loss: 0.3417\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0823 - accuracy: 0.9884 - graph_loss: 0.3463\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0714 - accuracy: 0.9898 - graph_loss: 0.3259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x206599cb8d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a new base MLP model.\n",
    "base_reg_model_tag, base_reg_model = 'FUNCTIONAL', make_mlp_functional_model(\n",
    "    HPARAMS)\n",
    "\n",
    "# Wrap the base MLP model with graph regularization.\n",
    "graph_reg_config = nsl.configs.make_graph_reg_config(\n",
    "    max_neighbors=HPARAMS.num_neighbors,\n",
    "    multiplier=HPARAMS.graph_regularization_multiplier,\n",
    "    distance_type=HPARAMS.distance_type,\n",
    "    sum_over_axis=-1)\n",
    "graph_reg_model = nsl.keras.GraphRegularization(base_reg_model,\n",
    "                                                graph_reg_config)\n",
    "graph_reg_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "graph_reg_model.fit(train_dataset, epochs=HPARAMS.train_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 52ms/step - loss: 1.0803 - accuracy: 0.8174 - graph_loss: 0.0000e+00\n",
      "\n",
      "\n",
      "Eval accuracy for  MLP + graph regularization :  0.81735986\n",
      "Eval loss for  MLP + graph regularization :  1.0802578389644624\n",
      "Eval graph loss for  MLP + graph regularization :  0.0\n"
     ]
    }
   ],
   "source": [
    "eval_results = dict(\n",
    "    zip(graph_reg_model.metrics_names,\n",
    "        graph_reg_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)))\n",
    "print_metrics('MLP + graph regularization', eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
