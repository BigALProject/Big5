{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.0.0\n",
      "Eager mode:  True\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "# Revised for our dataset from:\n",
    "# https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_mlp_cora\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import neural_structured_learning as nsl\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Resets notebook state\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### Experiment dataset\n",
    "TEST_DATA_PATH  = '../../data/filtered_images_for_training_tf/test.tfr'\n",
    "TRAIN_DATA_PATH = '../../data/filtered_images_for_training_tf/train.tfr'\n",
    "\n",
    "### Constants used to identify neighbor features in the input.\n",
    "NBR_FEATURE_PREFIX = 'NL_nbr_'\n",
    "NBR_WEIGHT_SUFFIX = '_weight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams(object):\n",
    "  def __init__(self):\n",
    "    # JDW set image size\n",
    "    #self.input_shape = [255, 255, 1]\n",
    "    self.input_shape = [1024, 1024, 1]\n",
    "    # JDW pass in max lenght used for default of 0\n",
    "    self.max_seq_length = self.input_shape[0] * self.input_shape[1] * self.input_shape[2]\n",
    "    # TODO JDW Set number of neighbors\n",
    "    self.num_neighbors = 1\n",
    "    # **************************************\n",
    "    self.num_classes = 4\n",
    "    # JDW Crashes with 64\n",
    "    #self.conv_filters = [32, 64, 64]\n",
    "    self.conv_filters = [32, 32, 32]\n",
    "    self.kernel_size = (3, 3)\n",
    "    self.pool_size = (2, 2)\n",
    "    self.num_fc_units = [64]\n",
    "    self.batch_size = 32\n",
    "    self.epochs = 5\n",
    "    self.adv_multiplier = 0.2\n",
    "    self.adv_step_size = 0.2\n",
    "    self.adv_grad_norm = 'infinity'\n",
    "\n",
    "HPARAMS = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_example(example_proto):\n",
    "  \"\"\"Extracts relevant fields from the `example_proto`.\n",
    "\n",
    "  Args:\n",
    "    example_proto: An instance of `tf.train.Example`.\n",
    "\n",
    "  Returns:\n",
    "    A pair whose first value is a dictionary containing relevant features\n",
    "    and whose second value contains the ground truth labels.\n",
    "  \"\"\"\n",
    "  # The 'image' feature is a image representation of the\n",
    "  # original raw image. A default value is required for examples that don't\n",
    "  # have the feature.\n",
    "  feature_spec = {\n",
    "      'image_x':\n",
    "          tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "      'image_y':\n",
    "          tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "      'image':\n",
    "          tf.io.FixedLenFeature([], tf.string),\n",
    "      'label':\n",
    "          tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "  }\n",
    "  # We also extract corresponding neighbor features in a similar manner to\n",
    "  # the features above.\n",
    "  #for i in range(HPARAMS.num_neighbors):\n",
    "  #  nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'image')\n",
    "  #  nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, i, NBR_WEIGHT_SUFFIX)\n",
    "  #  feature_spec[nbr_feature_key] = tf.io.FixedLenFeature(\n",
    "  #      [HPARAMS.max_seq_length],\n",
    "  #      tf.string)\n",
    "\n",
    "  # We assign a default value of 0.0 for the neighbor weight so that\n",
    "  # graph regularization is done on samples based on their exact number\n",
    "  # of neighbors. In other words, non-existent neighbors are discounted.\n",
    "  #feature_spec[nbr_weight_key] = tf.io.FixedLenFeature([1], tf.float32, default_value=tf.constant([0.0]))\n",
    "\n",
    "  #print(feature_spec.keys())\n",
    "  features = tf.io.parse_single_example(example_proto, feature_spec)\n",
    "\n",
    "  labels = features.pop('label')\n",
    "  return features, labels\n",
    "\n",
    "\n",
    "def make_dataset(file_path, training=False):\n",
    "  \"\"\"Creates a `tf.data.TFRecordDataset`.\n",
    "\n",
    "  Args:\n",
    "    file_path: Name of the file in the `.tfrecord` format containing\n",
    "      `tf.train.Example` objects.\n",
    "    training: Boolean indicating if we are in training mode.\n",
    "\n",
    "  Returns:\n",
    "    An instance of `tf.data.TFRecordDataset` containing the `tf.train.Example`\n",
    "    objects.\n",
    "  \"\"\"\n",
    "  dataset = tf.data.TFRecordDataset([file_path])\n",
    "  if training:\n",
    "    dataset = dataset.shuffle(10000)\n",
    "  dataset = dataset.map(parse_example)\n",
    "  dataset = dataset.batch(HPARAMS.batch_size)\n",
    "  return dataset\n",
    "\n",
    "# Make training and test sets\n",
    "train_dataset = make_dataset(TRAIN_DATA_PATH, training=True)\n",
    "test_dataset = make_dataset(TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ({image: (None,), image_x: (None,), image_y: (None,)}, (None,)), types: ({image: tf.string, image_x: tf.int64, image_y: tf.int64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def normalize(features):\n",
    "#  features[IMAGE_INPUT_NAME] = tf.cast(\n",
    "#      features[IMAGE_INPUT_NAME], dtype=tf.float32) / 255.0\n",
    "#  return features\n",
    "#\n",
    "#def convert_to_tuples(features):\n",
    "#  return features[IMAGE_INPUT_NAME], features[LABEL_INPUT_NAME]\n",
    "#\n",
    "#def convert_to_dictionaries(image, label):\n",
    "#  return {IMAGE_INPUT_NAME: image, LABEL_INPUT_NAME: label}\n",
    "#\n",
    "#train_dataset = train_dataset.map(normalize).shuffle(10000).batch(HPARAMS.batch_size).map(convert_to_tuples)\n",
    "#test_dataset = test_dataset.map(normalize).batch(HPARAMS.batch_size).map(convert_to_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think I want to do this instead\n",
    "## JDW \n",
    "#IMAGE_INPUT_NAME = \"image\"\n",
    "#\n",
    "#def build_base_model(hparams):\n",
    "#  \"\"\"Builds a model according to the architecture defined in `hparams`.\"\"\"\n",
    "#  inputs = tf.keras.Input(\n",
    "#      shape=hparams.input_shape, dtype=tf.float32, name=IMAGE_INPUT_NAME)\n",
    "#\n",
    "#  x = inputs\n",
    "#  for i, num_filters in enumerate(hparams.conv_filters):\n",
    "#    x = tf.keras.layers.Conv2D(\n",
    "#        num_filters, hparams.kernel_size, activation='relu')(x)\n",
    "#    if i < len(hparams.conv_filters) - 1:\n",
    "#     # max pooling between convolutional layers\n",
    "#      x = tf.keras.layers.MaxPooling2D(hparams.pool_size)(x)\n",
    "#  x = tf.keras.layers.Flatten()(x)\n",
    "#  for num_units in hparams.num_fc_units:\n",
    "#   x = tf.keras.layers.Dense(num_units, activation='relu', name=\"relu\")(x)\n",
    "#  pred = tf.keras.layers.Dense(hparams.num_classes, activation='softmax', name=\"softmax\")(x)\n",
    "#  model = tf.keras.Model(inputs=inputs, outputs=pred, name=\"Natural Disaster Analysis\")\n",
    "#  return model\n",
    "#\n",
    "## Generate Base Model\n",
    "#base_model = build_base_model(HPARAMS)\n",
    "#base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#base_model.fit(train_dataset, epochs=HPARAMS.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = base_model.evaluate(test_dataset)\n",
    "#named_results = dict(zip(base_model.metrics_names, results))\n",
    "#print('accuracy:', named_results['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for feature_batch, label_batch in train_dataset.take(1):\n",
    "  print('Feature list:', list(feature_batch.keys()))\n",
    "  print('Batch of inputs:', feature_batch['image'])\n",
    "  nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, 0, 'image')\n",
    "  nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, 0, NBR_WEIGHT_SUFFIX)\n",
    "  print('Batch of neighbor inputs:', feature_batch[nbr_feature_key])\n",
    "  print('Batch of neighbor weights:',\n",
    "        tf.reshape(feature_batch[nbr_weight_key], [-1]))\n",
    "  print('Batch of labels:', label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def make_mlp_functional_model(hparams):\n",
    "  \"\"\"Creates a functional API-based multi-layer perceptron model.\"\"\"\n",
    "  inputs = tf.keras.Input(\n",
    "      shape=(hparams.max_seq_length,), dtype='string', name='image')\n",
    "\n",
    "    # JDW\n",
    " # # Input is already one-hot encoded in the integer format. We cast it to\n",
    " # # floating point format here.\n",
    " # cur_layer = tf.keras.layers.Lambda(\n",
    " #     lambda x: tf.keras.backend.cast(x, tf.float32))(\n",
    " #         inputs)\n",
    "\n",
    "  for num_units in hparams.num_fc_units:\n",
    "    cur_layer = tf.keras.layers.Dense(num_units, activation='relu')(cur_layer)\n",
    "    # For functional models, by default, Keras ensures that the 'dropout' layer\n",
    "    # is invoked only during training.\n",
    "    cur_layer = tf.keras.layers.Dropout(hparams.dropout_rate)(cur_layer)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(\n",
    "      hparams.num_classes, activation='softmax')(\n",
    "          cur_layer)\n",
    "\n",
    "  model = tf.keras.Model(inputs, outputs=outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a base MLP model using the functional API.\n",
    "# Alternatively, you can also create a sequential or subclass base model using\n",
    "# the make_mlp_sequential_model() or make_mlp_subclass_model() functions\n",
    "# respectively, defined above. Note that if a subclass model is used, its\n",
    "# summary cannot be generated until it is built.\n",
    "base_model_tag, base_model = 'FUNCTIONAL', make_mlp_functional_model(HPARAMS)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Compile and train the base MLP model\n",
    "base_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "base_model.fit(train_dataset, epochs=HPARAMS.train_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to print evaluation metrics.\n",
    "def print_metrics(model_desc, eval_metrics):\n",
    "  \"\"\"Prints evaluation metrics.\n",
    "\n",
    "  Args:\n",
    "    model_desc: A description of the model.\n",
    "    eval_metrics: A dictionary mapping metric names to corresponding values. It\n",
    "      must contain the loss and accuracy metrics.\n",
    "  \"\"\"\n",
    "  print('\\n')\n",
    "  print('Eval accuracy for ', model_desc, ': ', eval_metrics['accuracy'])\n",
    "  print('Eval loss for ', model_desc, ': ', eval_metrics['loss'])\n",
    "  if 'graph_loss' in eval_metrics:\n",
    "    print('Eval graph loss for ', model_desc, ': ', eval_metrics['graph_loss'])\n",
    "\n",
    "eval_results = dict(\n",
    "    zip(base_model.metrics_names,\n",
    "        base_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)))\n",
    "print_metrics('Base MLP model', eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Build a new base MLP model.\n",
    "base_reg_model_tag, base_reg_model = 'FUNCTIONAL', make_mlp_functional_model(\n",
    "    HPARAMS)\n",
    "\n",
    "# Wrap the base MLP model with graph regularization.\n",
    "graph_reg_config = nsl.configs.make_graph_reg_config(\n",
    "    max_neighbors=HPARAMS.num_neighbors,\n",
    "    multiplier=HPARAMS.graph_regularization_multiplier,\n",
    "    distance_type=HPARAMS.distance_type,\n",
    "    sum_over_axis=-1)\n",
    "graph_reg_model = nsl.keras.GraphRegularization(base_reg_model,\n",
    "                                                graph_reg_config)\n",
    "graph_reg_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "graph_reg_model.fit(train_dataset, epochs=HPARAMS.train_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "eval_results = dict(\n",
    "    zip(graph_reg_model.metrics_names,\n",
    "        graph_reg_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)))\n",
    "print_metrics('MLP + graph regularization', eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
